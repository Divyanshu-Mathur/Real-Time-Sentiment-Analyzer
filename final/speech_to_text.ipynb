{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c7dd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data Science\\Multi Model Sentiment\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Data Science\\Multi Model Sentiment\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2Model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7926ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000          \n",
    "DURATION = 5.0                \n",
    "TEMP_WAV = \"user_record.wav\"   \n",
    "USE_GPU = torch.cuda.is_available()   \n",
    "DEVICE = \"cuda\" if USE_GPU else \"cpu\"      \n",
    "FUSION_MODEL_PATH = \"fusion_model.h5\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "211cba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASR_MODEL = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "EMBED_MODEL = \"facebook/wav2vec2-base-960h\"\n",
    "SENTENCE_BERT = \"all-MiniLM-L6-v2\" \n",
    "POOL_AUDIO = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22d60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(filename=TEMP_WAV, duration=DURATION, sr=SAMPLE_RATE):\n",
    "    print(f\"[record] Recording {duration}s of audio (sr={sr})... Speak now.\")\n",
    "    audio = sd.rec(int(duration * sr), samplerate=sr, channels=1, dtype=\"float32\")\n",
    "    sd.wait()\n",
    "    audio = audio.squeeze()\n",
    "    sf.write(filename, audio, sr, subtype='PCM_16')\n",
    "    print(f\"[record] Saved to {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "155e2895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data Science\\Multi Model Sentiment\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\.cache\\huggingface\\hub\\models--facebook--wav2vec2-large-960h-lv60-self. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading models...\")\n",
    "asr_processor = Wav2Vec2Processor.from_pretrained(ASR_MODEL)\n",
    "asr_model = Wav2Vec2ForCTC.from_pretrained(ASR_MODEL).to(DEVICE)\n",
    "asr_model.eval()\n",
    "\n",
    "embed_processor = Wav2Vec2Processor.from_pretrained(EMBED_MODEL)\n",
    "embed_model = Wav2Vec2Model.from_pretrained(EMBED_MODEL).to(DEVICE)\n",
    "embed_model.eval()\n",
    "\n",
    "sbert = SentenceTransformer(SENTENCE_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad78e7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data Science\\Multi Model Sentiment\\venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 6 variables whereas the saved optimizer has 46 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "d:\\Data Science\\Multi Model Sentiment\\venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 6 variables whereas the saved optimizer has 30 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "d:\\Data Science\\Multi Model Sentiment\\venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 12 variables whereas the saved optimizer has 22 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "\n",
    "with open(\"fusion_model_config.json\", \"r\") as f:\n",
    "    model_json = f.read()\n",
    "fusion_model = model_from_json(model_json)\n",
    "\n",
    "\n",
    "fusion_model.load_weights(\"fusion_model.weights.h5\")\n",
    "\n",
    "\n",
    "fusion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "def transcribe_wav(wav_path, asr_model, asr_processor, beam_width=10):\n",
    "\n",
    "    speech, sr = librosa.load(wav_path, sr=SAMPLE_RATE)\n",
    "    \n",
    "    speech = speech / max(1e-5, abs(speech).max())\n",
    "\n",
    "    input_values = asr_processor(speech, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=\"longest\").input_values.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = asr_model(input_values).logits\n",
    "\n",
    "    if hasattr(asr_processor, \"batch_decode\"):\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = asr_processor.batch_decode(predicted_ids)[0]\n",
    "    else:\n",
    "        from ctcdecode import CTCBeamDecoder\n",
    "        decoder = CTCBeamDecoder(\n",
    "            asr_processor.tokenizer.get_vocab(),\n",
    "            beam_width=beam_width,\n",
    "            blank_id=asr_processor.tokenizer.pad_token_id,\n",
    "            log_probs_input=True\n",
    "        )\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        beam_results, _, _, out_lens = decoder.decode(log_probs.cpu())\n",
    "        transcription = \"\".join([asr_processor.tokenizer.decode(beam_results[0][0][:out_lens[0][0]])])\n",
    "\n",
    "    transcription = transcription.lower().strip()\n",
    "    transcription = \" \".join(transcription.split()) \n",
    "\n",
    "    return transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4adf2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_embedding(wav_path, pool=True):\n",
    "    \"\"\"Return a 1D numpy vector embedding from wav2vec2 (mean pooled if pool=True).\"\"\"\n",
    "    speech, sr = librosa.load(wav_path, sr=SAMPLE_RATE)\n",
    "    input_values = embed_processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=\"longest\").input_values.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = embed_model(input_values)\n",
    "        last_hidden = outputs.last_hidden_state \n",
    "        if pool:\n",
    "            emb = last_hidden.mean(dim=1).squeeze().cpu().numpy()\n",
    "        else:\n",
    "            emb = last_hidden.squeeze().cpu().numpy()  \n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb96a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(text):\n",
    "    \"\"\"Return Sentence-BERT embedding (numpy 1D vector).\"\"\"\n",
    "    emb = sbert.encode([text], convert_to_numpy=True, show_progress_bar=False)[0]\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "938fbd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data Science\\Multi Model Sentiment\\venv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('audio_scaler.pkl', 'rb') as f:\n",
    "    audio_scaler  = pickle.load(f)\n",
    "\n",
    "with open('text_scaler.pkl', 'rb') as f:\n",
    "    text_scaler  = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae60c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from googletrans import Translator \n",
    "    HAS_GOOGLETRANS = True\n",
    "except Exception:\n",
    "    HAS_GOOGLETRANS = False\n",
    "translator = Translator() if HAS_GOOGLETRANS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9714d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_once(record_duration=DURATION, translate_to=None):\n",
    "\n",
    "    wav_path = record_audio(duration=record_duration)\n",
    "    SEQ_LEN = 16 \n",
    "    FEATURE_DIM = 48\n",
    "    \n",
    "    print(\"[step] Transcribing audio...\")\n",
    "    try:\n",
    "        text = transcribe_wav(wav_path, asr_model, asr_processor)\n",
    "    except Exception as e:\n",
    "        print(\"[error] Transcription failed:\", e)\n",
    "        text = \"\"\n",
    "    print(\"Transcribed text:\", repr(text))\n",
    "\n",
    "    translated = None\n",
    "    text_for_embedding = text\n",
    "    if translator:\n",
    "        try:\n",
    "            translated = translator.translate(text, dest='en').text\n",
    "            print(\"Translated (en) for embedding:\", translated)\n",
    "            text_for_embedding = translated\n",
    "        except Exception as e:\n",
    "            print(\"[warning] Translation failed:\", e)\n",
    "\n",
    "    print(\"[step] Extracting audio embedding...\")\n",
    "    audio_emb = get_audio_embedding(wav_path, pool=POOL_AUDIO)\n",
    "    print(\"Raw audio emb shape:\", audio_emb.shape)\n",
    "    audio_emb_seq = audio_emb.reshape(1, SEQ_LEN, FEATURE_DIM)\n",
    "\n",
    "    if len(text_for_embedding.strip()) == 0:\n",
    "        text_emb = np.zeros((sbert.get_sentence_embedding_dimension(),))\n",
    "    else:\n",
    "        text_emb = get_text_embedding(text_for_embedding)\n",
    "    print(\"Text emb shape:\", text_emb.shape)\n",
    "    text_emb_seq = text_emb.reshape(1, -1)\n",
    "\n",
    "    global audio_scaler, text_scaler\n",
    "    if audio_scaler is not None:\n",
    "        audio_emb_seq = audio_scaler.transform(audio_emb_seq.reshape(1, -1)).reshape(1, SEQ_LEN, FEATURE_DIM)\n",
    "    if text_scaler is not None:\n",
    "        text_emb_seq = text_scaler.transform(text_emb_seq)\n",
    "\n",
    "    if fusion_model is None:\n",
    "        print(\"[info] No fusion model available to predict emotion.\")\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"translated\": translated,\n",
    "            \"audio_emb\": audio_emb,\n",
    "            \"text_emb\": text_emb,\n",
    "            \"prediction\": None\n",
    "        }\n",
    "\n",
    "    print(\"[step] Predicting emotion...\")\n",
    "    pred_prob = fusion_model.predict([audio_emb_seq, text_emb_seq], verbose=0)[0]\n",
    "    pred_idx = np.argmax(pred_prob)\n",
    "\n",
    "    try:\n",
    "        label_classes = fusion_model.class_names\n",
    "    except Exception:\n",
    "        label_classes = ['anger', 'joy', 'neutral', 'sadness']\n",
    "\n",
    "    pred_label = label_classes[pred_idx] if pred_idx < len(label_classes) else str(pred_idx)\n",
    "    print(f\"Predicted emotion: {pred_label} (prob={pred_prob[pred_idx]:.3f})\")\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"translated\": translated,\n",
    "        \"audio_emb\": audio_emb,\n",
    "        \"text_emb\": text_emb,\n",
    "        \"prediction\": {\n",
    "            \"label\": pred_label,\n",
    "            \"prob\": float(pred_prob[pred_idx]),\n",
    "            \"probs\": pred_prob.tolist()\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d261d785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[record] Recording 5.0s of audio (sr=16000)... Speak now.\n",
      "[record] Saved to user_record.wav\n",
      "[step] Transcribing audio...\n",
      "Transcribed text: 'i'\n",
      "Translated (en) for embedding: i\n",
      "[step] Extracting audio embedding...\n",
      "Raw audio emb shape: (768,)\n",
      "Text emb shape: (384,)\n",
      "[step] Predicting emotion...\n",
      "Predicted emotion: neutral (prob=0.527)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    result = infer_once(record_duration=DURATION, translate_to='hi')\n",
    "    text = result['text'].lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e52e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
