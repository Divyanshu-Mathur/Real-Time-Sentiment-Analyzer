{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03355269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data Science\\Multi Model Sentiment\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ed2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_FOLDER = \"audio_output_4_final\"\n",
    "METADATA_CSV = \"audio_metadata_filtered_4_final.csv\"\n",
    "SR = 16000\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099059c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(METADATA_CSV)\n",
    "df = df[['wav_filename','emotion','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e84449bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Wav2Vec2GroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): Wav2Vec2FeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Wav2Vec2Encoder(\n",
       "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): Wav2Vec2SamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(DEVICE)\n",
    "wav2vec_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d0d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_embedding(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=SR)\n",
    "    input_values = processor(y, sampling_rate=sr, return_tensors=\"pt\").input_values.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = wav2vec_model(input_values)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1)  \n",
    "    return embedding.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b3a8483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20/2933 audio files\n",
      "Processed 40/2933 audio files\n",
      "Processed 60/2933 audio files\n",
      "Processed 80/2933 audio files\n",
      "Processed 100/2933 audio files\n",
      "Processed 120/2933 audio files\n",
      "Processed 140/2933 audio files\n",
      "Processed 160/2933 audio files\n",
      "Processed 180/2933 audio files\n",
      "Processed 200/2933 audio files\n",
      "Processed 220/2933 audio files\n",
      "Processed 240/2933 audio files\n",
      "Processed 260/2933 audio files\n",
      "Processed 280/2933 audio files\n",
      "Processed 300/2933 audio files\n",
      "Processed 320/2933 audio files\n",
      "Processed 340/2933 audio files\n",
      "Processed 360/2933 audio files\n",
      "Processed 380/2933 audio files\n",
      "Processed 400/2933 audio files\n",
      "Processed 420/2933 audio files\n",
      "Processed 440/2933 audio files\n",
      "Processed 460/2933 audio files\n",
      "Processed 480/2933 audio files\n",
      "Processed 500/2933 audio files\n",
      "Processed 520/2933 audio files\n",
      "Processed 540/2933 audio files\n",
      "Processed 560/2933 audio files\n",
      "Processed 580/2933 audio files\n",
      "Processed 600/2933 audio files\n",
      "Processed 620/2933 audio files\n",
      "Processed 640/2933 audio files\n",
      "Processed 660/2933 audio files\n",
      "Processed 680/2933 audio files\n",
      "Processed 700/2933 audio files\n",
      "Processed 720/2933 audio files\n",
      "Processed 740/2933 audio files\n",
      "Processed 760/2933 audio files\n",
      "Processed 780/2933 audio files\n",
      "Processed 800/2933 audio files\n",
      "Processed 820/2933 audio files\n",
      "Processed 840/2933 audio files\n",
      "Processed 860/2933 audio files\n",
      "Processed 880/2933 audio files\n",
      "Processed 900/2933 audio files\n",
      "Processed 920/2933 audio files\n",
      "Processed 940/2933 audio files\n",
      "Processed 960/2933 audio files\n",
      "Processed 980/2933 audio files\n",
      "Processed 1000/2933 audio files\n",
      "Processed 1020/2933 audio files\n",
      "Processed 1040/2933 audio files\n",
      "Processed 1060/2933 audio files\n",
      "Processed 1080/2933 audio files\n",
      "Processed 1100/2933 audio files\n",
      "Processed 1120/2933 audio files\n",
      "Processed 1140/2933 audio files\n",
      "Processed 1160/2933 audio files\n",
      "Processed 1180/2933 audio files\n",
      "Processed 1200/2933 audio files\n",
      "Processed 1220/2933 audio files\n",
      "Processed 1240/2933 audio files\n",
      "Processed 1260/2933 audio files\n",
      "Processed 1280/2933 audio files\n",
      "Processed 1300/2933 audio files\n",
      "Processed 1320/2933 audio files\n",
      "Processed 1340/2933 audio files\n",
      "Processed 1360/2933 audio files\n",
      "Processed 1380/2933 audio files\n",
      "Processed 1400/2933 audio files\n",
      "Processed 1420/2933 audio files\n",
      "Processed 1440/2933 audio files\n",
      "Processed 1460/2933 audio files\n",
      "Processed 1480/2933 audio files\n",
      "Processed 1500/2933 audio files\n",
      "Processed 1520/2933 audio files\n",
      "Processed 1540/2933 audio files\n",
      "Processed 1560/2933 audio files\n",
      "Processed 1580/2933 audio files\n",
      "Processed 1600/2933 audio files\n",
      "Processed 1620/2933 audio files\n",
      "Processed 1640/2933 audio files\n",
      "Processed 1660/2933 audio files\n",
      "Processed 1680/2933 audio files\n",
      "Processed 1700/2933 audio files\n",
      "Processed 1720/2933 audio files\n",
      "Processed 1740/2933 audio files\n",
      "Processed 1760/2933 audio files\n",
      "Processed 1780/2933 audio files\n",
      "Processed 1800/2933 audio files\n",
      "Processed 1820/2933 audio files\n",
      "Processed 1840/2933 audio files\n",
      "Processed 1860/2933 audio files\n",
      "Processed 1880/2933 audio files\n",
      "Processed 1900/2933 audio files\n",
      "Processed 1920/2933 audio files\n",
      "Processed 1940/2933 audio files\n",
      "Processed 1960/2933 audio files\n",
      "Processed 1980/2933 audio files\n",
      "Processed 2000/2933 audio files\n",
      "Processed 2020/2933 audio files\n",
      "Processed 2040/2933 audio files\n",
      "Processed 2060/2933 audio files\n",
      "Processed 2080/2933 audio files\n",
      "Processed 2100/2933 audio files\n",
      "Processed 2120/2933 audio files\n",
      "Processed 2140/2933 audio files\n",
      "Processed 2160/2933 audio files\n",
      "Processed 2180/2933 audio files\n",
      "Processed 2200/2933 audio files\n",
      "Processed 2220/2933 audio files\n",
      "Processed 2240/2933 audio files\n",
      "Processed 2260/2933 audio files\n",
      "Processed 2280/2933 audio files\n",
      "Processed 2300/2933 audio files\n",
      "Processed 2320/2933 audio files\n",
      "Processed 2340/2933 audio files\n",
      "Processed 2360/2933 audio files\n",
      "Processed 2380/2933 audio files\n",
      "Processed 2400/2933 audio files\n",
      "Processed 2420/2933 audio files\n",
      "Processed 2440/2933 audio files\n",
      "Processed 2460/2933 audio files\n",
      "Processed 2480/2933 audio files\n",
      "Processed 2500/2933 audio files\n",
      "Processed 2520/2933 audio files\n",
      "Processed 2540/2933 audio files\n",
      "Processed 2560/2933 audio files\n",
      "Processed 2580/2933 audio files\n",
      "Processed 2600/2933 audio files\n",
      "Processed 2620/2933 audio files\n",
      "Processed 2640/2933 audio files\n",
      "Processed 2660/2933 audio files\n",
      "Processed 2680/2933 audio files\n",
      "Processed 2700/2933 audio files\n",
      "Processed 2720/2933 audio files\n",
      "Processed 2740/2933 audio files\n",
      "Processed 2760/2933 audio files\n",
      "Processed 2780/2933 audio files\n",
      "Processed 2800/2933 audio files\n",
      "Processed 2820/2933 audio files\n",
      "Processed 2840/2933 audio files\n",
      "Processed 2860/2933 audio files\n",
      "Processed 2880/2933 audio files\n",
      "Processed 2900/2933 audio files\n",
      "Processed 2920/2933 audio files\n",
      "Audio embeddings shape: (2933, 768)\n"
     ]
    }
   ],
   "source": [
    "audio_embeddings = []\n",
    "for idx, row in df.iterrows():\n",
    "    wav_path = os.path.join(AUDIO_FOLDER, row['wav_filename'])\n",
    "    emb = get_audio_embedding(wav_path)\n",
    "    audio_embeddings.append(emb)\n",
    "    if (idx+1) % 20 == 0:\n",
    "        print(f\"Processed {idx+1}/{len(df)} audio files\")\n",
    "\n",
    "audio_embeddings = np.array(audio_embeddings)\n",
    "print(\"Audio embeddings shape:\", audio_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f361d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved audio features\n"
     ]
    }
   ],
   "source": [
    "audio_df = pd.DataFrame(audio_embeddings)\n",
    "audio_df['emotion'] = df['emotion']\n",
    "audio_df['wav_filename'] = df['wav_filename']\n",
    "audio_df.to_csv('audio_features_4.csv', index=False)\n",
    "print(f\"Saved audio features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b69ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cce6617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Data Science\\Multi Model Sentiment\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 184/184 [00:13<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embeddings shape: (2933, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')  \n",
    "text_embeddings = bert_model.encode(df['text'].tolist(), batch_size=16, show_progress_bar=True)\n",
    "\n",
    "print(\"Text embeddings shape:\", text_embeddings.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e76ac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved text features\n"
     ]
    }
   ],
   "source": [
    "text_df = pd.DataFrame(text_embeddings)\n",
    "text_df['emotion'] = df['emotion'].values\n",
    "text_df.to_csv('text_features_4.csv', index=False)\n",
    "print(f\"Saved text features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f604f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined feature shape: (2933, 1152)\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack([audio_embeddings, text_embeddings])\n",
    "print(\"Combined feature shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23395777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined embeddings\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.DataFrame(X)\n",
    "combined_df['emotion'] = df['emotion']\n",
    "combined_df['wav_filename'] = df['wav_filename']\n",
    "combined_df.to_csv('audio_text_features_4.csv', index=False)\n",
    "print(f\"Saved combined embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e41a6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
